{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning - Top 5 Algorithms\n",
    "\n",
    "**Objective:** Fine-tune the best performing algorithms to maximize MAPE improvement.\n",
    "\n",
    "**Algorithms to Tune:**\n",
    "1. Gradient Boosting (MAPE: 1.1872) - BEST OVERALL\n",
    "2. CatBoost (MAPE: 1.2604) - Best for 5 targets\n",
    "3. XGBoost (MAPE: 1.3327) - Best for 3 targets\n",
    "4. LightGBM (MAPE: 1.3136) - Consistent performer\n",
    "5. Random Forest (MAPE: 1.9521) - Diversity + fallback\n",
    "\n",
    "**Workflow:**\n",
    "1. Load data\n",
    "2. Document baseline results\n",
    "3. Tune each algorithm with GridSearchCV\n",
    "4. Compare tuned vs baseline\n",
    "5. Save best hyperparameters for final training\n",
    "\n",
    "**Note:** This notebook will take 10-30 minutes to run (depending on hardware)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas numpy scikit-learn xgboost lightgbm catboost -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import time\n",
    "from sklearn.model_selection import cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('✓ All libraries imported successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data loaded successfully\n",
      "  Training set: (2000, 65)\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "train_path = '/home/reu24mandaloju/projects/shell_ai_hack/data/train.csv'  \n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "\n",
    "print('✓ Data loaded successfully')\n",
    "print(f'  Training set: {train_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Features and targets prepared\n",
      "  Features: (2000, 55)\n",
      "  Targets: (2000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Identify feature and target columns\n",
    "blend_cols = [col for col in train_df.columns if 'fraction' in col]\n",
    "component_prop_cols = [col for col in train_df.columns if 'Component' in col and 'Property' in col]\n",
    "target_cols = [col for col in train_df.columns if 'BlendProperty' in col]\n",
    "\n",
    "features_cols = blend_cols + component_prop_cols\n",
    "\n",
    "# Prepare training data\n",
    "X_train = train_df[features_cols].values\n",
    "y_train = train_df[target_cols].values\n",
    "\n",
    "print('✓ Features and targets prepared')\n",
    "print(f'  Features: {X_train.shape}')\n",
    "print(f'  Targets: {y_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline Results Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BASELINE RESULTS (from 03_model_experiments.ipynb)\n",
      "================================================================================\n",
      "\n",
      "Algorithm                 Baseline MAPE   Ranking   \n",
      "--------------------------------------------------------------------------------\n",
      "Gradient Boosting         1.1872          1\n",
      "CatBoost                  1.2604          2\n",
      "XGBoost                   1.3327          3\n",
      "LightGBM                  1.3136          4\n",
      "Random Forest             1.9521          5\n"
     ]
    }
   ],
   "source": [
    "# Baseline results from 03_model_experiments.ipynb\n",
    "baseline_results = {\n",
    "    'Gradient Boosting': 1.1872,\n",
    "    'CatBoost': 1.2604,\n",
    "    'XGBoost': 1.3327,\n",
    "    'LightGBM': 1.3136,\n",
    "    'Random Forest': 1.9521\n",
    "}\n",
    "\n",
    "tuned_results = {}\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('BASELINE RESULTS (from 03_model_experiments.ipynb)')\n",
    "print('='*80)\n",
    "print(f'\\n{\"Algorithm\":<25} {\"Baseline MAPE\":<15} {\"Ranking\":<10}')\n",
    "print('-'*80)\n",
    "\n",
    "for rank, (algo, mape) in enumerate(baseline_results.items(), 1):\n",
    "    print(f'{algo:<25} {mape:<15.4f} {rank}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Tesla V100-DGXS-32GB\n"
     ]
    }
   ],
   "source": [
    "# Check if NVIDIA GPU is available\n",
    "import torch\n",
    "print(torch.cuda.is_available())  # Should print: True\n",
    "print(torch.cuda.get_device_name(0))  # Should print your GPU name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning - Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HYPERPARAMETER TUNING: GRADIENT BOOSTING\n",
      "================================================================================\n",
      "\n",
      "GridSearchCV running for each target...\n",
      "Parameter combinations: 162 to test\n",
      "  BlendProperty1: Best MAPE = 1.0318 (Time: 190.0s)\n",
      "  BlendProperty2: Best MAPE = 0.8187 (Time: 176.4s)\n",
      "  BlendProperty3: Best MAPE = 1.0122 (Time: 181.6s)\n",
      "  BlendProperty4: Best MAPE = 0.9398 (Time: 182.9s)\n",
      "  BlendProperty5: Best MAPE = 0.0402 (Time: 189.3s)\n",
      "  BlendProperty6: Best MAPE = 0.8234 (Time: 178.6s)\n",
      "  BlendProperty7: Best MAPE = 1.6154 (Time: 174.0s)\n",
      "  BlendProperty8: Best MAPE = 0.9797 (Time: 179.4s)\n",
      "  BlendProperty9: Best MAPE = 1.0424 (Time: 183.1s)\n",
      "  BlendProperty10: Best MAPE = 0.9116 (Time: 181.6s)\n",
      "\n",
      "✓ Gradient Boosting Tuning Complete\n",
      "  Baseline MAPE: 1.1872\n",
      "  Tuned MAPE:    0.9215\n",
      "  Improvement:   +0.2657 (+22.38%)\n"
     ]
    }
   ],
   "source": [
    "print('\\n' + '='*80)\n",
    "print('HYPERPARAMETER TUNING: GRADIENT BOOSTING')\n",
    "print('='*80)\n",
    "\n",
    "# Define parameter grid for Gradient Boosting\n",
    "gb_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "# Base model\n",
    "gb_base = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Store results for each target\n",
    "gb_best_params_per_target = {}\n",
    "gb_best_scores = []\n",
    "\n",
    "print(f'\\nGridSearchCV running for each target...')\n",
    "print(f'Parameter combinations: {np.prod(list(map(len, gb_param_grid.values())))} to test')\n",
    "\n",
    "for target_idx, target_name in enumerate(target_cols):\n",
    "    y_target = y_train[:, target_idx]\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        gb_base,\n",
    "        gb_param_grid,\n",
    "        cv=5,\n",
    "        scoring='neg_mean_absolute_percentage_error',\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    grid_search.fit(X_train, y_target)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    best_mape = -grid_search.best_score_\n",
    "    gb_best_params_per_target[target_name] = grid_search.best_params_\n",
    "    gb_best_scores.append(best_mape)\n",
    "    \n",
    "    print(f'  {target_name}: Best MAPE = {best_mape:.4f} (Time: {elapsed:.1f}s)')\n",
    "\n",
    "gb_overall_mape = np.mean(gb_best_scores)\n",
    "gb_improvement = baseline_results['Gradient Boosting'] - gb_overall_mape\n",
    "gb_improvement_pct = (gb_improvement / baseline_results['Gradient Boosting']) * 100\n",
    "\n",
    "print(f'\\n✓ Gradient Boosting Tuning Complete')\n",
    "print(f'  Baseline MAPE: {baseline_results[\"Gradient Boosting\"]:.4f}')\n",
    "print(f'  Tuned MAPE:    {gb_overall_mape:.4f}')\n",
    "print(f'  Improvement:   {gb_improvement:+.4f} ({gb_improvement_pct:+.2f}%)')\n",
    "\n",
    "tuned_results['Gradient Boosting'] = gb_overall_mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Tuning - CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HYPERPARAMETER TUNING: CATBOOST (GPU-ACCELERATED)\n",
      "================================================================================\n",
      "\n",
      "GridSearchCV running for each target (GPU-accelerated)...\n",
      "Parameter combinations: 81 to test\n",
      "  BlendProperty1: Best MAPE = 1.0787 (Time: 982.2s)\n",
      "  BlendProperty2: Best MAPE = 0.6191 (Time: 974.8s)\n",
      "  BlendProperty3: Best MAPE = 1.0741 (Time: 976.5s)\n",
      "  BlendProperty4: Best MAPE = 0.6091 (Time: 974.4s)\n",
      "  BlendProperty5: Best MAPE = 0.1998 (Time: 976.3s)\n",
      "  BlendProperty6: Best MAPE = 0.6529 (Time: 976.2s)\n",
      "  BlendProperty7: Best MAPE = 1.1402 (Time: 977.4s)\n",
      "  BlendProperty8: Best MAPE = 1.0038 (Time: 976.2s)\n",
      "  BlendProperty9: Best MAPE = 0.9664 (Time: 975.5s)\n",
      "  BlendProperty10: Best MAPE = 0.5276 (Time: 976.9s)\n",
      "\n",
      "✓ CatBoost Tuning Complete (GPU-Accelerated)\n",
      "  Baseline MAPE: 1.2604\n",
      "  Tuned MAPE:    0.7872\n",
      "  Improvement:   +0.4732 (+37.55%)\n"
     ]
    }
   ],
   "source": [
    "print('\\n' + '='*80)\n",
    "print('HYPERPARAMETER TUNING: CATBOOST (GPU-ACCELERATED)')\n",
    "print('='*80)\n",
    "\n",
    "# Define parameter grid for CatBoost\n",
    "cat_param_grid = {\n",
    "    'iterations': [50, 100, 200],\n",
    "    'learning_rate': [0.03, 0.1, 0.2],\n",
    "    'depth': [4, 6, 8],\n",
    "    'l2_leaf_reg': [1, 3, 5]\n",
    "}\n",
    "\n",
    "# Base model WITH GPU\n",
    "cat_base = CatBoostRegressor(\n",
    "    random_state=42, \n",
    "    verbose=False,\n",
    "    task_type='GPU'  # ← ADD THIS LINE\n",
    ")\n",
    "\n",
    "# Store results for each target\n",
    "cat_best_params_per_target = {}\n",
    "cat_best_scores = []\n",
    "\n",
    "print(f'\\nGridSearchCV running for each target (GPU-accelerated)...')\n",
    "print(f'Parameter combinations: {np.prod(list(map(len, cat_param_grid.values())))} to test')\n",
    "\n",
    "for target_idx, target_name in enumerate(target_cols):\n",
    "    y_target = y_train[:, target_idx]\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        cat_base,\n",
    "        cat_param_grid,\n",
    "        cv=5,\n",
    "        scoring='neg_mean_absolute_percentage_error',\n",
    "        n_jobs=1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    grid_search.fit(X_train, y_target)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    best_mape = -grid_search.best_score_\n",
    "    cat_best_params_per_target[target_name] = grid_search.best_params_\n",
    "    cat_best_scores.append(best_mape)\n",
    "    \n",
    "    print(f'  {target_name}: Best MAPE = {best_mape:.4f} (Time: {elapsed:.1f}s)')\n",
    "\n",
    "cat_overall_mape = np.mean(cat_best_scores)\n",
    "cat_improvement = baseline_results['CatBoost'] - cat_overall_mape\n",
    "cat_improvement_pct = (cat_improvement / baseline_results['CatBoost']) * 100\n",
    "\n",
    "print(f'\\n✓ CatBoost Tuning Complete (GPU-Accelerated)')\n",
    "print(f'  Baseline MAPE: {baseline_results[\"CatBoost\"]:.4f}')\n",
    "print(f'  Tuned MAPE:    {cat_overall_mape:.4f}')\n",
    "print(f'  Improvement:   {cat_improvement:+.4f} ({cat_improvement_pct:+.2f}%)')\n",
    "\n",
    "tuned_results['CatBoost'] = cat_overall_mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Tuning - XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HYPERPARAMETER TUNING: XGBOOST (CPU-OPTIMIZED)\n",
      "================================================================================\n",
      "\n",
      "GridSearchCV running for each target...\n",
      "Parameter combinations: 243 to test\n",
      "  BlendProperty1: Best MAPE = 0.9552 (Time: 87.4s)\n",
      "  BlendProperty2: Best MAPE = 0.7346 (Time: 87.4s)\n",
      "  BlendProperty3: Best MAPE = 1.0855 (Time: 85.0s)\n",
      "  BlendProperty4: Best MAPE = 0.8224 (Time: 85.7s)\n",
      "  BlendProperty5: Best MAPE = 0.0614 (Time: 72.0s)\n",
      "  BlendProperty6: Best MAPE = 0.7925 (Time: 85.6s)\n",
      "  BlendProperty7: Best MAPE = 1.1035 (Time: 84.4s)\n",
      "  BlendProperty8: Best MAPE = 1.0576 (Time: 85.2s)\n",
      "  BlendProperty9: Best MAPE = 1.0212 (Time: 84.7s)\n",
      "  BlendProperty10: Best MAPE = 0.7161 (Time: 86.8s)\n",
      "\n",
      "✓ XGBoost Tuning Complete\n",
      "  Baseline MAPE: 1.3327\n",
      "  Tuned MAPE:    0.8350\n",
      "  Improvement:   +0.4977 (+37.35%)\n"
     ]
    }
   ],
   "source": [
    "print('\\n' + '='*80)\n",
    "print('HYPERPARAMETER TUNING: XGBOOST (CPU-OPTIMIZED)')\n",
    "print('='*80)\n",
    "\n",
    "# Define parameter grid for XGBoost\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Base model (CPU-optimized)\n",
    "xgb_base = XGBRegressor(\n",
    "    random_state=42, \n",
    "    verbosity=0,\n",
    "    tree_method='hist'  # ← CPU-optimized histogram method\n",
    ")\n",
    "\n",
    "# Store results for each target\n",
    "xgb_best_params_per_target = {}\n",
    "xgb_best_scores = []\n",
    "\n",
    "print(f'\\nGridSearchCV running for each target...')\n",
    "print(f'Parameter combinations: {np.prod(list(map(len, xgb_param_grid.values())))} to test')\n",
    "\n",
    "for target_idx, target_name in enumerate(target_cols):\n",
    "    y_target = y_train[:, target_idx]\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        xgb_base,\n",
    "        xgb_param_grid,\n",
    "        cv=5,\n",
    "        scoring='neg_mean_absolute_percentage_error',\n",
    "        n_jobs=-1,  # ← CPU parallelization (safe for CPU tree_method)\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    grid_search.fit(X_train, y_target)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    best_mape = -grid_search.best_score_\n",
    "    xgb_best_params_per_target[target_name] = grid_search.best_params_\n",
    "    xgb_best_scores.append(best_mape)\n",
    "    \n",
    "    print(f'  {target_name}: Best MAPE = {best_mape:.4f} (Time: {elapsed:.1f}s)')\n",
    "\n",
    "xgb_overall_mape = np.mean(xgb_best_scores)\n",
    "xgb_improvement = baseline_results['XGBoost'] - xgb_overall_mape\n",
    "xgb_improvement_pct = (xgb_improvement / baseline_results['XGBoost']) * 100\n",
    "\n",
    "print(f'\\n✓ XGBoost Tuning Complete')\n",
    "print(f'  Baseline MAPE: {baseline_results[\"XGBoost\"]:.4f}')\n",
    "print(f'  Tuned MAPE:    {xgb_overall_mape:.4f}')\n",
    "print(f'  Improvement:   {xgb_improvement:+.4f} ({xgb_improvement_pct:+.2f}%)')\n",
    "\n",
    "tuned_results['XGBoost'] = xgb_overall_mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Hyperparameter Tuning - LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HYPERPARAMETER TUNING: LIGHTGBM (GPU-ACCELERATED)\n",
      "================================================================================\n",
      "\n",
      "GridSearchCV running for each target (GPU-accelerated)...\n",
      "Parameter combinations: 243 to test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 warning warning generated generated.\n",
      ".\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated1.\n",
      " warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning1 warning generated.\n",
      " generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated1 warning generated.\n",
      ".\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "11 warning generated.\n",
      " warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "11 warning generated.\n",
      " warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "11 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      " warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "11 warning generated.\n",
      " warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "11 warning generated.\n",
      " warning generated.\n",
      "1 warning generated.\n",
      "11 warning generated.\n",
      " warning generated.\n",
      "1 warning generated.\n",
      "11 warning generated.\n",
      " warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "11 warning generated.\n",
      " warning generated.\n",
      "11 warning generated.\n",
      "1 warning generated.\n",
      " warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "11 warning generated.\n",
      " warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n",
      "1 warning generated.\n"
     ]
    }
   ],
   "source": [
    "print('\\n' + '='*80)\n",
    "print('HYPERPARAMETER TUNING: LIGHTGBM (GPU-ACCELERATED)')\n",
    "print('='*80)\n",
    "\n",
    "# Define parameter grid for LightGBM\n",
    "lgb_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'num_leaves': [20, 31, 50],\n",
    "    'subsample': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Base model WITH GPU\n",
    "lgb_base = LGBMRegressor(\n",
    "    random_state=42, \n",
    "    verbose=-1,\n",
    "    device_type='gpu'  # ← ADD THIS\n",
    ")\n",
    "\n",
    "# Store results for each target\n",
    "lgb_best_params_per_target = {}\n",
    "lgb_best_scores = []\n",
    "\n",
    "print(f'\\nGridSearchCV running for each target (GPU-accelerated)...')\n",
    "print(f'Parameter combinations: {np.prod(list(map(len, lgb_param_grid.values())))} to test')\n",
    "\n",
    "for target_idx, target_name in enumerate(target_cols):\n",
    "    y_target = y_train[:, target_idx]\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        lgb_base,\n",
    "        lgb_param_grid,\n",
    "        cv=5,\n",
    "        scoring='neg_mean_absolute_percentage_error',\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    grid_search.fit(X_train, y_target)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    best_mape = -grid_search.best_score_\n",
    "    lgb_best_params_per_target[target_name] = grid_search.best_params_\n",
    "    lgb_best_scores.append(best_mape)\n",
    "    \n",
    "    print(f'  {target_name}: Best MAPE = {best_mape:.4f} (Time: {elapsed:.1f}s)')\n",
    "\n",
    "lgb_overall_mape = np.mean(lgb_best_scores)\n",
    "lgb_improvement = baseline_results['LightGBM'] - lgb_overall_mape\n",
    "lgb_improvement_pct = (lgb_improvement / baseline_results['LightGBM']) * 100\n",
    "\n",
    "print(f'\\n✓ LightGBM Tuning Complete (GPU-Accelerated)')\n",
    "print(f'  Baseline MAPE: {baseline_results[\"LightGBM\"]:.4f}')\n",
    "print(f'  Tuned MAPE:    {lgb_overall_mape:.4f}')\n",
    "print(f'  Improvement:   {lgb_improvement:+.4f} ({lgb_improvement_pct:+.2f}%)')\n",
    "\n",
    "tuned_results['LightGBM'] = lgb_overall_mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Hyperparameter Tuning - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*80)\n",
    "print('HYPERPARAMETER TUNING: RANDOM FOREST')\n",
    "print('='*80)\n",
    "\n",
    "# Define parameter grid for Random Forest\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Base model\n",
    "rf_base = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "# Store results for each target\n",
    "rf_best_params_per_target = {}\n",
    "rf_best_scores = []\n",
    "\n",
    "print(f'\\nGridSearchCV running for each target...')\n",
    "print(f'Parameter combinations: {np.prod(list(map(len, rf_param_grid.values())))} to test')\n",
    "\n",
    "for target_idx, target_name in enumerate(target_cols):\n",
    "    y_target = y_train[:, target_idx]\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        rf_base,\n",
    "        rf_param_grid,\n",
    "        cv=5,\n",
    "        scoring='neg_mean_absolute_percentage_error',\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    grid_search.fit(X_train, y_target)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    best_mape = -grid_search.best_score_\n",
    "    rf_best_params_per_target[target_name] = grid_search.best_params_\n",
    "    rf_best_scores.append(best_mape)\n",
    "    \n",
    "    print(f'  {target_name}: Best MAPE = {best_mape:.4f} (Time: {elapsed:.1f}s)')\n",
    "\n",
    "rf_overall_mape = np.mean(rf_best_scores)\n",
    "rf_improvement = baseline_results['Random Forest'] - rf_overall_mape\n",
    "rf_improvement_pct = (rf_improvement / baseline_results['Random Forest']) * 100\n",
    "\n",
    "print(f'\\n✓ Random Forest Tuning Complete')\n",
    "print(f'  Baseline MAPE: {baseline_results[\"Random Forest\"]:.4f}')\n",
    "print(f'  Tuned MAPE:    {rf_overall_mape:.4f}')\n",
    "print(f'  Improvement:   {rf_improvement:+.4f} ({rf_improvement_pct:+.2f}%)')\n",
    "\n",
    "tuned_results['Random Forest'] = rf_overall_mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Compare Baseline vs Tuned Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BASELINE vs TUNED RESULTS COMPARISON\n",
      "================================================================================\n",
      "\n",
      "Algorithm                 Baseline     Tuned        Improvement     Status         \n",
      "--------------------------------------------------------------------------------\n",
      "Gradient Boosting         1.1872       0.9215       +0.2657 (+22.38%) ✓ Improved     \n",
      "CatBoost                  1.2604       0.7872       +0.4732 (+37.55%) ✓ Improved     \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'XGBoost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m algo \u001b[38;5;129;01min\u001b[39;00m baseline_results\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m      9\u001b[0m     baseline \u001b[38;5;241m=\u001b[39m baseline_results[algo]\n\u001b[0;32m---> 10\u001b[0m     tuned \u001b[38;5;241m=\u001b[39m \u001b[43mtuned_results\u001b[49m\u001b[43m[\u001b[49m\u001b[43malgo\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     11\u001b[0m     improvement \u001b[38;5;241m=\u001b[39m baseline \u001b[38;5;241m-\u001b[39m tuned\n\u001b[1;32m     12\u001b[0m     improvement_pct \u001b[38;5;241m=\u001b[39m (improvement \u001b[38;5;241m/\u001b[39m baseline) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'XGBoost'"
     ]
    }
   ],
   "source": [
    "print('\\n' + '='*80)\n",
    "print('BASELINE vs TUNED RESULTS COMPARISON')\n",
    "print('='*80)\n",
    "\n",
    "print(f'\\n{\"Algorithm\":<25} {\"Baseline\":<12} {\"Tuned\":<12} {\"Improvement\":<15} {\"Status\":<15}')\n",
    "print('-'*80)\n",
    "\n",
    "for algo in baseline_results.keys():\n",
    "    baseline = baseline_results[algo]\n",
    "    tuned = tuned_results[algo]\n",
    "    improvement = baseline - tuned\n",
    "    improvement_pct = (improvement / baseline) * 100\n",
    "    \n",
    "    if improvement > 0:\n",
    "        status = f'✓ Improved'\n",
    "    elif improvement == 0:\n",
    "        status = '= No change'\n",
    "    else:\n",
    "        status = '⚠️ Worse'\n",
    "    \n",
    "    print(f'{algo:<25} {baseline:<12.4f} {tuned:<12.4f} {improvement:+.4f} ({improvement_pct:+.2f}%) {status:<15}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'XGBoost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m algos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(baseline_results\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m      5\u001b[0m baseline_vals \u001b[38;5;241m=\u001b[39m [baseline_results[algo] \u001b[38;5;28;01mfor\u001b[39;00m algo \u001b[38;5;129;01min\u001b[39;00m algos]\n\u001b[0;32m----> 6\u001b[0m tuned_vals \u001b[38;5;241m=\u001b[39m [tuned_results[algo] \u001b[38;5;28;01mfor\u001b[39;00m algo \u001b[38;5;129;01min\u001b[39;00m algos]\n\u001b[1;32m      8\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(algos))\n\u001b[1;32m      9\u001b[0m width \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.35\u001b[39m\n",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m algos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(baseline_results\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m      5\u001b[0m baseline_vals \u001b[38;5;241m=\u001b[39m [baseline_results[algo] \u001b[38;5;28;01mfor\u001b[39;00m algo \u001b[38;5;129;01min\u001b[39;00m algos]\n\u001b[0;32m----> 6\u001b[0m tuned_vals \u001b[38;5;241m=\u001b[39m [\u001b[43mtuned_results\u001b[49m\u001b[43m[\u001b[49m\u001b[43malgo\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m algo \u001b[38;5;129;01min\u001b[39;00m algos]\n\u001b[1;32m      8\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(algos))\n\u001b[1;32m      9\u001b[0m width \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.35\u001b[39m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'XGBoost'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualization: Baseline vs Tuned\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "algos = list(baseline_results.keys())\n",
    "baseline_vals = [baseline_results[algo] for algo in algos]\n",
    "tuned_vals = [tuned_results[algo] for algo in algos]\n",
    "\n",
    "x = np.arange(len(algos))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, baseline_vals, width, label='Baseline', color='steelblue', alpha=0.8, edgecolor='black')\n",
    "bars2 = ax.bar(x + width/2, tuned_vals, width, label='Tuned', color='green', alpha=0.8, edgecolor='black')\n",
    "\n",
    "ax.set_ylabel('MAPE', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Hyperparameter Tuning Results - Baseline vs Tuned', fontsize=13, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(algos, rotation=45, ha='right')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.axhline(y=2.72, color='red', linestyle='--', linewidth=2, label='Reference Baseline')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('✓ Comparison visualization displayed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Best Hyperparameters Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BEST HYPERPARAMETERS FOR FINAL TRAINING\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lgb_best_params_per_target' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 9\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBEST HYPERPARAMETERS FOR FINAL TRAINING\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m      5\u001b[0m all_best_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGradient Boosting\u001b[39m\u001b[38;5;124m'\u001b[39m: gb_best_params_per_target,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCatBoost\u001b[39m\u001b[38;5;124m'\u001b[39m: cat_best_params_per_target,\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXGBoost\u001b[39m\u001b[38;5;124m'\u001b[39m: xgb_best_params_per_target,\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLightGBM\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mlgb_best_params_per_target\u001b[49m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandom Forest\u001b[39m\u001b[38;5;124m'\u001b[39m: rf_best_params_per_target\n\u001b[1;32m     11\u001b[0m }\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m algo_name, params_dict \u001b[38;5;129;01min\u001b[39;00m all_best_params\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00malgo_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lgb_best_params_per_target' is not defined"
     ]
    }
   ],
   "source": [
    "print('\\n' + '='*80)\n",
    "print('BEST HYPERPARAMETERS FOR FINAL TRAINING')\n",
    "print('='*80)\n",
    "\n",
    "all_best_params = {\n",
    "    'Gradient Boosting': gb_best_params_per_target,\n",
    "    'CatBoost': cat_best_params_per_target,\n",
    "    'XGBoost': xgb_best_params_per_target,\n",
    "    'LightGBM': lgb_best_params_per_target,\n",
    "    'Random Forest': rf_best_params_per_target\n",
    "}\n",
    "\n",
    "for algo_name, params_dict in all_best_params.items():\n",
    "    print(f'\\n{algo_name}:')\n",
    "    print(f'  Tuned MAPE: {tuned_results[algo_name]:.4f}')\n",
    "    print(f'  Best hyperparameters per target:')\n",
    "    for target_name, params in params_dict.items():\n",
    "        print(f'    {target_name}:')\n",
    "        for param_name, param_value in params.items():\n",
    "            print(f'      - {param_name}: {param_value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*80)\n",
    "print('HYPERPARAMETER TUNING COMPLETE')\n",
    "print('='*80)\n",
    "\n",
    "print(f'\\n✓ TUNING RESULTS SUMMARY:')\n",
    "print(f'\\n  Algorithms tuned: 5')\n",
    "print(f'  Targets per algorithm: 10')\n",
    "print(f'  Total grid searches: 50')\n",
    "\n",
    "# Calculate overall improvements\n",
    "overall_baseline = np.mean(list(baseline_results.values()))\n",
    "overall_tuned = np.mean(list(tuned_results.values()))\n",
    "overall_improvement = overall_baseline - overall_tuned\n",
    "overall_improvement_pct = (overall_improvement / overall_baseline) * 100\n",
    "\n",
    "print(f'\\n  Overall Baseline MAPE: {overall_baseline:.4f}')\n",
    "print(f'  Overall Tuned MAPE:    {overall_tuned:.4f}')\n",
    "print(f'  Overall Improvement:   {overall_improvement:+.4f} ({overall_improvement_pct:+.2f}%)')\n",
    "\n",
    "# Find best algorithm\n",
    "best_algo = min(tuned_results, key=tuned_results.get)\n",
    "best_mape = tuned_results[best_algo]\n",
    "\n",
    "print(f'\\n✓ BEST ALGORITHM (After Tuning):')\n",
    "print(f'  Algorithm: {best_algo}')\n",
    "print(f'  Tuned MAPE: {best_mape:.4f}')\n",
    "print(f'  vs Reference Baseline (2.72): {2.72 - best_mape:+.4f}')\n",
    "print(f'  Performance Gain: {((2.72 - best_mape) / 2.72 * 100):.2f}%')\n",
    "\n",
    "print(f'\\n📋 NEXT STEPS:')\n",
    "print(f'  1. Review best hyperparameters above')\n",
    "print(f'  2. Use these parameters in 05_final_training_and_predictions.ipynb')\n",
    "print(f'  3. Retrain models on full training data')\n",
    "print(f'  4. Generate test predictions')\n",
    "print(f'  5. Create ensemble if desired')\n",
    "print(f'  6. Submit predictions')\n",
    "\n",
    "print(f'\\n' + '='*80)\n",
    "print('Ready for final training and predictions!')\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
