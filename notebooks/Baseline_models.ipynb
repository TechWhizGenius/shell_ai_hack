{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains only the modeling part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0-py3-none-win_amd64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\tm0792.students.007\\onedrive - unt system\\competitions\\shell ai hackathon\\.venv\\lib\\site-packages (from lightgbm) (2.3.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\tm0792.students.007\\onedrive - unt system\\competitions\\shell ai hackathon\\.venv\\lib\\site-packages (from lightgbm) (1.16.0)\n",
      "Downloading lightgbm-4.6.0-py3-none-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 13.3 MB/s eta 0:00:00\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the training and test datasets\n",
    "train_df = pd.read_csv(r\"C:\\Users\\tm0792.STUDENTS.007\\OneDrive - UNT System\\Competitions\\Shell ai Hackathon\\dataset\\train.csv\")\n",
    "test_df = pd.read_csv(r\"C:\\Users\\tm0792.STUDENTS.007\\OneDrive - UNT System\\Competitions\\Shell ai Hackathon\\dataset\\test.csv\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing Baseline models:\n",
    "Linear Regression (MultiOutputRegressor) \n",
    "Random Forest Regressor \n",
    "Gradient Boosting (XGBoost/LightGBM)\n",
    "\n",
    "\n",
    "At first implementing these models with k-fold cross validation and see whether there is any overfitting or not. \n",
    "\n",
    "Later, I wouuld like to implement k-fold cross validation technique and see the difference\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = train_df.iloc[:, :55]\n",
    "y_train = train_df.iloc[:, 55:]\n",
    "\n",
    "X_test = test_df.iloc[:, :55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression MAPE: 2.2956\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "multi_lr = MultiOutputRegressor(lr)\n",
    "multi_lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lr = multi_lr.predict(X_train)\n",
    "mape_lr = mean_absolute_percentage_error(y_train, y_pred_lr)\n",
    "print(f\"Linear Regression MAPE: {mape_lr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest MAPE: 0.7768\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "multi_rf = MultiOutputRegressor(rf)\n",
    "multi_rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = multi_rf.predict(X_train)\n",
    "mape_rf = mean_absolute_percentage_error(y_train, y_pred_rf)\n",
    "print(f\"Random Forest MAPE: {mape_rf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000887 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12982\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 55\n",
      "[LightGBM] [Info] Start training from score -0.016879\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001192 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12982\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 55\n",
      "[LightGBM] [Info] Start training from score -0.002076\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000675 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12982\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 55\n",
      "[LightGBM] [Info] Start training from score -0.014351\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000667 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12982\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 55\n",
      "[LightGBM] [Info] Start training from score -0.006068\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000804 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12982\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 55\n",
      "[LightGBM] [Info] Start training from score -0.015249\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001223 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12982\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 55\n",
      "[LightGBM] [Info] Start training from score -0.003497\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000680 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12982\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 55\n",
      "[LightGBM] [Info] Start training from score -0.013568\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000672 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12982\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 55\n",
      "[LightGBM] [Info] Start training from score -0.017236\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000715 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12982\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 55\n",
      "[LightGBM] [Info] Start training from score -0.001507\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000623 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12982\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 55\n",
      "[LightGBM] [Info] Start training from score -0.001795\n",
      "LightGBM MAPE: 0.3541\n"
     ]
    }
   ],
   "source": [
    "lgb_model = lgb.LGBMRegressor(n_estimators=100, random_state=42)\n",
    "multi_lgb = MultiOutputRegressor(lgb_model)\n",
    "multi_lgb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lgb = multi_lgb.predict(X_train)\n",
    "mape_lgb = mean_absolute_percentage_error(y_train, y_pred_lgb)\n",
    "print(f\"LightGBM MAPE: {mape_lgb:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can say that, lightgbm>random forest>linear regression. \n",
    "\n",
    "Note:\n",
    "\n",
    "we usually split the data into training and validation sets (or use K-Fold) to estimate generalization performance.\n",
    "\n",
    "The reason I trained and evaluated on the full dataset was to give a quick baseline — a fast way to see if the models can learn anything useful at all from the data.\n",
    "\n",
    "Now that we’ve confirmed the models work, the next correct and necessary step is to apply K-Fold cross-validation or a proper train/validation split to get a realistic performance estimate and tune the models properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
